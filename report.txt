Full report to analyze my data:

Data information:
Data Information Report
Problem Type: Binary classification
Target Variable: target
Feature Types: {'numeric': ['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'feat_64', 'feat_65', 'feat_66', 'feat_67', 'feat_68', 'feat_69', 'feat_70', 'feat_71', 'feat_72', 'feat_73', 'feat_74', 'feat_75', 'feat_76', 'feat_77', 'feat_78', 'feat_79', 'feat_80', 'feat_81', 'feat_82', 'feat_83', 'feat_84', 'feat_85', 'feat_86', 'feat_87', 'feat_88', 'feat_89', 'feat_90', 'feat_91', 'feat_92', 'feat_93', 'feat_94', 'feat_95', 'feat_96', 'feat_97', 'feat_98', 'feat_99'], 'boolean': [], 'text': [], 'categorical': [], 'time': [], 'id': []}
Loss Function: Cross entropy
Evaluation Metrics: ['Accuracy', 'Precision', 'Recall', 'F1']
Optional Models: {'baseline-mode', 'catboost', 'logistic'}


features engineering process for the data:
features statistics for the data. Analyzed by groups (text, numeric, categorical) if exists:

Numerical Feature statistics:
Feature stats:
            col # unique # missing                 max                  min  \
63  num_feat_63       28         0   5.239999771118164   -2.309999942779541   
79  num_feat_79       28         0  3.0299999713897705  -1.8600000143051147   
80  num_feat_80       28         0  2.8499999046325684  -2.4200000762939453   
50  num_feat_50       28         0   5.320000171661377   -4.440000057220459   
88  num_feat_88       28         0   2.559999942779541  -2.7100000381469727   
31  num_feat_31       28         0    4.53000020980835   -7.150000095367432   
40  num_feat_40       28         0   4.949999809265137    -4.28000020980835   
25  num_feat_25       28         0   6.909999847412109   -7.900000095367432   
26  num_feat_26       28         0  1.2999999523162842    -9.15999984741211   
38  num_feat_38       28         0   6.119999885559082   -7.340000152587891   

                     avg                 std              CI  \
63     1.309999942779541  1.9299999475479126     (0.71, 1.9)   
79   0.14000000059604645  1.0299999713897705   (-0.18, 0.46)   
80   0.07000000029802322  1.3200000524520874   (-0.34, 0.48)   
50  -0.23999999463558197   2.009999990463257   (-0.87, 0.38)   
88   -0.6399999856948853   1.059999942779541  (-0.97, -0.31)   
31    -3.259999990463257  2.8299999237060547  (-4.14, -2.38)   
40    -0.800000011920929  2.1500000953674316  (-1.47, -0.13)   
25    -2.309999942779541   3.890000104904175   (-3.52, -1.1)   
26   -3.4600000381469727   3.190000057220459  (-4.46, -2.47)   
38   -1.5399999618530273   2.859999895095825  (-2.43, -0.65)   

                  median         top_vals        bottom_vals  
63    1.2200000286102295    5.24,4.5,4.26   -2.31,-2.1,-1.94  
79  0.029999999329447746   3.03,1.56,1.41  -1.86,-1.57,-1.06  
80   0.20000000298023224   2.85,2.28,1.73  -2.42,-2.29,-1.73  
50  0.009999999776482582   5.32,2.56,2.44  -4.44,-3.57,-2.76  
88    -0.800000011920929   2.56,1.31,0.79  -2.71,-1.89,-1.89  
31   -3.0199999809265137  4.53,0.87,-0.38   -7.15,-6.95,-6.7  
40   -1.1799999475479126   4.95,2.35,2.31  -4.28,-3.59,-3.12  
25   -2.8399999141693115   6.91,5.34,3.49   -7.9,-7.01,-6.17  
26   -3.2300000190734863    1.3,1.25,1.15  -9.16,-8.87,-8.38  
38                 -1.75   6.12,1.76,1.58  -7.34,-6.34,-5.43  


Categorical Feature statistics:
** model pre analysis report:
Feature stats:
            col # unique # missing       Distribution   Sample  \
0  target_label        2         0  yes: 57%, no: 43%  yes, no   

  Top 5 values coverage  
0                  100%  

values counts for categorical target to predict:
yes    16
no     12
Name: target, dtype: int64

top 50 with lowest partial pvalue for numerical feat, based on chi square:
[('num_feat_10', 1.0), ('num_feat_86', 1.0), ('num_feat_60', 1.0), ('num_feat_63', 1.0), ('num_feat_95', 1.0), ('num_feat_6', 1.0), ('num_feat_32', 1.0), ('num_feat_22', 1.0), ('num_feat_55', 1.0), ('num_feat_45', 1.0), ('num_feat_82', 1.0), ('num_feat_89', 1.0), ('num_feat_26', 1.0), ('num_feat_69', 1.0), ('num_feat_12', 1.0), ('num_feat_46', 1.0), ('num_feat_23', 1.0), ('num_feat_59', 1.0), ('num_feat_75', 1.0), ('num_feat_25', 1.0), ('num_feat_37', 1.0), ('num_feat_13', 1.0), ('num_feat_80', 1.0), ('num_feat_3', 1.0), ('num_feat_4', 1.0), ('num_feat_31', 1.0), ('num_feat_41', 1.0), ('num_feat_87', 1.0), ('num_feat_93', 1.0), ('num_feat_42', 1.0), ('num_feat_67', 1.0), ('num_feat_28', 1.0), ('num_feat_90', 1.0), ('num_feat_84', 1.0), ('num_feat_21', 1.0), ('num_feat_8', 1.0), ('num_feat_64', 1.0), ('num_feat_0', 1.0), ('num_feat_11', 1.0), ('num_feat_40', 1.0), ('num_feat_48', 1.0), ('num_feat_2', 1.0), ('num_feat_39', 1.0), ('num_feat_43', 1.0), ('num_feat_19', 1.0), ('num_feat_71', 1.0), ('num_feat_24', 1.0), ('num_feat_54', 1.0), ('num_feat_9', 1.0), ('num_feat_20', 1.0)]

Training models and choosing the best one
Model results compared - showing the best out of each type after CV & HP tuning: 
+-------+-------------------------+--------------------+--------------------+--------------------+--------------------+
|  type |          model          |   accuracy_score   |  precision_score   |    recall_score    |      f1_score      |
+-------+-------------------------+--------------------+--------------------+--------------------+--------------------+
|  test |      BaselineModel      |        0.63        |        0.63        |        1.0         | 0.7506349206349207 |
|  test |      CatboostModel      |        0.58        | 0.6799999999999999 | 0.8666666666666666 | 0.6792063492063491 |
|  test | LogisticRegressionModel |        0.67        |        0.67        | 0.6666666666666666 | 0.6214285714285714 |
| train |      BaselineModel      | 0.6359477124183007 | 0.6359477124183007 |        1.0         | 0.7764812166369452 |
| train |      CatboostModel      |        1.0         |        1.0         |        1.0         |        1.0         |
| train | LogisticRegressionModel |        1.0         |        1.0         |        1.0         |        1.0         |
+-------+-------------------------+--------------------+--------------------+--------------------+--------------------+


Post model analysis - analyzing results of the chosen model: 
confusion matrix on the test set
[[1 3]
 [0 2]]

Learning curve by training examples
+-------------+-------------------+------------------+---------------------+---------------------+
| train_sizes | train_scores_mean | train_scores_std |   test_scores_mean  |   test_scores_std   |
+-------------+-------------------+------------------+---------------------+---------------------+
|      1      |        nan        |       nan        |         nan         |         nan         |
|      5      |        1.0        |       0.0        | 0.47000000000000003 |  0.1777638883463118 |
|      9      |        1.0        |       0.0        |         0.5         | 0.06324555320336757 |
|      13     |        1.0        |       0.0        |         0.64        | 0.09695359714832659 |
|      17     |        1.0        |       0.0        |         0.67        | 0.18867962264113208 |
+-------------+-------------------+------------------+---------------------+---------------------+


Boostrap metrics, sample 20 results:
+----------------+---------------------+--------------+--------------------+
| accuracy_score |   precision_score   | recall_score |      f1_score      |
+----------------+---------------------+--------------+--------------------+
|      0.5       |  0.3939393939393939 |     1.0      | 0.5652173913043478 |
|     0.515      | 0.40853658536585363 |     1.0      | 0.5800865800865801 |
|      0.51      |  0.3717948717948718 |     1.0      | 0.5420560747663552 |
|     0.535      | 0.41139240506329117 |     1.0      | 0.5829596412556054 |
|      0.5       | 0.38650306748466257 |     1.0      | 0.5575221238938053 |
|     0.505      | 0.39634146341463417 |     1.0      | 0.5676855895196506 |
|     0.495      | 0.40236686390532544 |     1.0      | 0.5738396624472574 |
|     0.475      | 0.38235294117647056 |     1.0      | 0.5531914893617021 |
|     0.525      |  0.4311377245508982 |     1.0      | 0.602510460251046  |
|      0.52      |  0.4251497005988024 |     1.0      | 0.596638655462185  |
|      0.49      |         0.4         |     1.0      | 0.5714285714285715 |
|      0.54      |  0.4588235294117647 |     1.0      | 0.6290322580645161 |
|     0.505      | 0.40718562874251496 |     1.0      | 0.5787234042553191 |
|      0.49      | 0.41379310344827586 |     1.0      | 0.5853658536585366 |
|      0.54      | 0.46511627906976744 |     1.0      | 0.6349206349206349 |
|     0.515      |       0.39375       |     1.0      | 0.5650224215246636 |
|      0.54      | 0.42857142857142855 |     1.0      |        0.6         |
|     0.515      |  0.4581005586592179 |     1.0      | 0.6283524904214559 |
|      0.53      |        0.4125       |     1.0      | 0.5840707964601769 |
|      0.51      |  0.4367816091954023 |     1.0      |       0.608        |
+----------------+---------------------+--------------+--------------------+

Trying to find weaknesses in the model by training models on the error:
Most important features by catboost:
+-------------+------------+
|   Feature   | Importance |
+-------------+------------+
| num_feat_18 |    12.0    |
| num_feat_36 |    8.0     |
| num_feat_33 |    7.0     |
| num_feat_64 |    6.0     |
|  num_feat_9 |    5.0     |
| num_feat_94 |    4.0     |
|  num_feat_8 |    4.0     |
| num_feat_85 |    4.0     |
|  num_feat_7 |    3.0     |
| num_feat_40 |    3.0     |
| num_feat_32 |    3.0     |
| num_feat_37 |    3.0     |
| num_feat_30 |    3.0     |
| num_feat_39 |    3.0     |
| num_feat_15 |    3.0     |
|  num_feat_0 |    3.0     |
| num_feat_86 |    2.0     |
| num_feat_31 |    2.0     |
| num_feat_90 |    2.0     |
| num_feat_96 |    2.0     |
| num_feat_62 |    2.0     |
| num_feat_43 |    1.0     |
| num_feat_47 |    1.0     |
| num_feat_48 |    1.0     |
| num_feat_67 |    1.0     |
| num_feat_65 |    1.0     |
| num_feat_71 |    1.0     |
| num_feat_79 |    1.0     |
| num_feat_87 |    1.0     |
|  num_feat_3 |    1.0     |
| num_feat_20 |    1.0     |
| num_feat_13 |    1.0     |
| num_feat_98 |    1.0     |
| num_feat_81 |    1.0     |
| num_feat_63 |    0.0     |
| num_feat_97 |    0.0     |
| num_feat_66 |    0.0     |
| num_feat_72 |    0.0     |
| num_feat_95 |    0.0     |
| num_feat_68 |    0.0     |
| num_feat_69 |    0.0     |
| num_feat_70 |    0.0     |
| num_feat_93 |    0.0     |
| num_feat_92 |    0.0     |
| num_feat_80 |    0.0     |
| num_feat_91 |    0.0     |
| num_feat_73 |    0.0     |
| num_feat_60 |    0.0     |
| num_feat_74 |    0.0     |
| num_feat_75 |    0.0     |
+-------------+------------+

Print of tree to look for potential segments with high error (use with caution)
|--- num_feat_43 <= -0.63
|   |--- value: [0.00]
|--- num_feat_43 >  -0.63
|   |--- value: [1.00]


