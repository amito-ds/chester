Full report to analyze my data:

Data information:
Data Information Report
Problem Type: Binary classification
Target Variable: target
Feature Types: {'numeric': ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History'], 'boolean': [], 'text': [], 'categorical': ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'property_Area'], 'time': [], 'id': ['Loan_ID']}
Loss Function: Cross entropy
Evaluation Metrics: ['Accuracy', 'Precision', 'Recall', 'F1']
Optional Models: {'catboost', 'logistic', 'baseline-mode'}


features engineering process for the data:
features statistics for the data. Analyzed by groups (text, numeric, categorical) if exists:

Numerical Feature statistics:

Categorical Feature statistics:
Feature stats:
             col # unique # missing                       Distribution  \
3      Education        2         1  Graduate: 78%, Not Graduate: 2...   
1  property_Area        3         0  Semiurban: 38%, Urban: 33%, Ru...   
5     Dependents        4        15     0: 58%, 1: 17%, 2: 17%, 3+: 9%   
0  Self_Employed        2        32                  No: 86%, Yes: 14%   
4         Gender        2        15             Male: 81%, Female: 19%   
2        Married        2         3                  Yes: 65%, No: 35%   

                    Sample Top 5 values coverage  
3   Graduate, Not Graduate                  100%  
1  Urban, Semiurban, Rural                  100%  
5                 0, 3+, 1                  100%  
0                  No, Yes                  100%  
4             Male, Female                  100%  
2                  Yes, No                  100%  

** model pre analysis report:
Feature stats:
            col # unique # missing    Distribution Sample  \
0  target_label        2         0  Y: 69%, N: 31%   N, Y   

  Top 5 values coverage  
0                  100%  

values counts for categorical target to predict:
Y    422
N    192
Name: target, dtype: int64

top 50 with lowest partial pvalue for categorical feat based on chi square:
[('property_Area', 0.0021360187811644937), ('Married', 0.03439381301579988), ('Education', 0.04465356518805114), ('Dependents', 0.3678506740863211), ('Gender', 0.7284755861881353), ('Self_Employed', 1.0)]

Training models and choosing the best one
Model results compared - showing the best out of each type after CV & HP tuning: 
+-------+-------------------------+--------------------+--------------------+--------------------+--------------------+
|  type |          model          |   accuracy_score   |  precision_score   |    recall_score    |      f1_score      |
+-------+-------------------------+--------------------+--------------------+--------------------+--------------------+
|  test |      BaselineModel      | 0.6925376211090497 | 0.6925376211090497 |        1.0         | 0.8169918283326056 |
|  test |      CatboostModel      | 0.8125747268604412 | 0.7999015449156219 | 0.9701193251193251 | 0.8761315192128561 |
|  test | LogisticRegressionModel | 0.6925376211090497 | 0.6925376211090497 |        1.0         | 0.8169918283326056 |
| train |      BaselineModel      | 0.6924689723217531 | 0.6924689723217531 |        1.0         | 0.8182081809425202 |
| train |      CatboostModel      | 0.8299358674767617 | 0.8108431314156066 | 0.9838001070827508 | 0.8889765970896933 |
| train | LogisticRegressionModel | 0.6924689723217531 | 0.6924689723217531 |        1.0         | 0.8182081809425202 |
+-------+-------------------------+--------------------+--------------------+--------------------+--------------------+


Post model analysis - analyzing results of the chosen model: 
Top 100 Feature importance (in %, 100 = the most important one):
{'num_Credit_History': 100, 'num_LoanAmount': 17, 'index_property_Area': 10, 'num_ApplicantIncome': 7, 'num_CoapplicantIncome': 7, 'index_Dependents': 5, 'index_Married': 4, 'index_Education': 4, 'index_Self_Employed': 3, 'num_Loan_Amount_Term': 2, 'index_Gender': 2}

confusion matrix on the test set
[[18 23]
 [ 3 79]]

Learning curve by training examples
+-------------+--------------------+-----------------------+--------------------+----------------------+
| train_sizes | train_scores_mean  |    train_scores_std   |  test_scores_mean  |   test_scores_std    |
+-------------+--------------------+-----------------------+--------------------+----------------------+
|      39     | 0.9076923076923077 |  0.020512820512820485 | 0.7147804576376006 | 0.044708880519352644 |
|     127     | 0.8866141732283465 |  0.018365202818410364 | 0.8126159554730983 | 0.02473175574691366  |
|     215     | 0.866046511627907  |  0.009018939269611781 | 0.8044114615543186 | 0.03286133807078102  |
|     303     | 0.8554455445544555 | 0.0032336498254563256 | 0.8044114615543186 | 0.03759071749904322  |
|     392     | 0.8341836734693878 |  0.006248698323426459 | 0.8084930942073798 | 0.028762461338653515 |
+-------------+--------------------+-----------------------+--------------------+----------------------+


Boostrap metrics, sample 20 results:
+----------------+--------------------+--------------------+--------------------+
| accuracy_score |  precision_score   |    recall_score    |      f1_score      |
+----------------+--------------------+--------------------+--------------------+
|     0.805      | 0.7848837209302325 | 0.9854014598540146 | 0.8737864077669902 |
|     0.845      | 0.838150289017341  | 0.9797297297297297 | 0.9034267912772586 |
|      0.78      | 0.7515151515151515 | 0.9763779527559056 | 0.8493150684931506 |
|      0.7       | 0.6975308641975309 | 0.9112903225806451 | 0.7902097902097903 |
|      0.79      | 0.7696969696969697 | 0.9694656488549618 | 0.8581081081081081 |
|      0.84      | 0.8490566037735849 | 0.9440559440559441 | 0.8940397350993377 |
|      0.78      | 0.7810650887573964 | 0.9496402877697842 | 0.8571428571428571 |
|      0.79      | 0.7682926829268293 | 0.9692307692307692 | 0.8571428571428572 |
|      0.8       | 0.7948717948717948 | 0.9393939393939394 | 0.8611111111111112 |
|     0.785      |       0.7625       | 0.9606299212598425 | 0.8501742160278746 |
|     0.795      | 0.7777777777777778 | 0.9618320610687023 | 0.8600682593856656 |
|      0.8       | 0.7831325301204819 | 0.9701492537313433 | 0.8666666666666668 |
|      0.76      | 0.7396449704142012 | 0.9689922480620154 | 0.8389261744966443 |
|      0.82      | 0.8023952095808383 | 0.9781021897810219 | 0.8815789473684211 |
|      0.77      | 0.7543859649122807 | 0.9699248120300752 | 0.8486842105263159 |
|     0.805      | 0.7818181818181819 | 0.9772727272727273 | 0.8686868686868686 |
|      0.82      | 0.8203592814371258 | 0.958041958041958  | 0.8838709677419354 |
|     0.775      | 0.7724550898203593 | 0.9485294117647058 | 0.8514851485148515 |
|     0.815      | 0.8048780487804879 | 0.9635036496350365 | 0.877076411960133  |
|     0.805      | 0.7852760736196319 | 0.9696969696969697 | 0.8677966101694916 |
+----------------+--------------------+--------------------+--------------------+

Trying to find weaknesses in the model by training models on the error:
Most important features by catboost:
+-----------------------+------------+
|        Feature        | Importance |
+-----------------------+------------+
|  num_ApplicantIncome  |    27.0    |
|     num_LoanAmount    |    24.0    |
| num_CoapplicantIncome |    12.0    |
|    index_Education    |    8.0     |
|  index_property_Area  |    7.0     |
|      index_Gender     |    5.0     |
|  index_Self_Employed  |    5.0     |
|    index_Dependents   |    4.0     |
|   num_Credit_History  |    4.0     |
|     index_Married     |    2.0     |
|  num_Loan_Amount_Term |    2.0     |
+-----------------------+------------+

Print of tree to look for potential segments with high error (use with caution)
|--- index_Education_1 <= 0.50
|   |--- index_property_Area_0 <= 131.00
|   |   |--- value: [0.21]
|   |--- index_property_Area_0 >  131.00
|   |   |--- value: [0.37]
|--- index_Education_1 >  0.50
|   |--- index_Self_Employed_0 <= 4312.50
|   |   |--- value: [0.04]
|   |--- index_Self_Employed_0 >  4312.50
|   |   |--- value: [0.20]


