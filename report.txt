Full report to analyze my data:

Data information:
Data Information Report
Problem Type: Multiclass classification
Target Variable: target
Feature Types: {'numeric': {'feat_4', 'feat_11', 'feat_31', 'feat_19', 'feat_1', 'feat_22', 'feat_3', 'feat_28', 'feat_0', 'feat_36', 'feat_16', 'feat_27', 'feat_2', 'feat_25', 'feat_23', 'feat_8', 'feat_7', 'feat_29', 'feat_32', 'feat_10', 'feat_14', 'feat_9', 'feat_18', 'feat_33', 'feat_17', 'feat_21', 'feat_38', 'feat_6', 'feat_30', 'feat_37', 'feat_34', 'feat_20', 'feat_26', 'feat_35', 'feat_24', 'feat_15', 'feat_12', 'feat_39', 'feat_5', 'feat_13'}, 'boolean': set(), 'text': set(), 'categorical': set(), 'time': set(), 'other': set()}
Loss Function: Cross entropy
Evaluation Metrics: ['Accuracy', 'Precision', 'Recall', 'F1']
Optional Models: {'catboost', 'baseline-mode', 'logistic'}


features engineering process for the data:
** model pre analysis report:
Feature stats:
            col # unique # missing                       Distribution  \
0  target_label        3         0  classic: 46%, disco: 31%, pop:...   

                Sample Top 5 values coverage  
0  disco, classic, pop                  100%  

values counts for categorical target to predict:
classic    22
disco      15
pop        11
Name: target, dtype: int64

Training models and choosing the best one
Model results compared - showing the best out of each type after CV & HP tuning: 
+-------+-------------------------+---------------------+--------------------+--------------------+--------------------+------+
|  type |          model          |    accuracy_score   |  precision_score   |    recall_score    |      f1_score      | None |
+-------+-------------------------+---------------------+--------------------+--------------------+--------------------+------+
|  test |      BaselineModel      |  0.4714285714285714 |        0.0         |        0.0         |        0.0         | nan  |
|  test |      CatboostModel      |  0.9714285714285713 | 0.9777777777777779 | 0.9666666666666666 | 0.9644444444444444 | nan  |
|  test | LogisticRegressionModel |        0.975        | 0.9833333333333332 | 0.9777777777777779 | 0.9771428571428572 | nan  |
| train |      BaselineModel      | 0.47354838709677416 |        nan         |        nan         |        nan         | nan  |
| train |      CatboostModel      |         1.0         |        1.0         |        1.0         |        1.0         | nan  |
| train | LogisticRegressionModel |         1.0         |        1.0         |        1.0         |        1.0         | nan  |
+-------+-------------------------+---------------------+--------------------+--------------------+--------------------+------+


Post model analysis - analyzing results of the chosen model: 
confusion matrix on the test set
[[4 0 0]
 [0 3 0]
 [1 0 2]]

Learning curve by training examples
+-------------+-------------------+------------------+--------------------+----------------------+
| train_sizes | train_scores_mean | train_scores_std |  test_scores_mean  |   test_scores_std    |
+-------------+-------------------+------------------+--------------------+----------------------+
|      3      |        nan        |       nan        |        nan         |         nan          |
|      9      |        1.0        |       0.0        | 0.7928571428571429 | 0.16659862556700858  |
|      16     |        1.0        |       0.0        |        1.0         |         0.0          |
|      23     |        1.0        |       0.0        |       0.975        | 0.049999999999999996 |
|      30     |        1.0        |       0.0        |       0.925        | 0.09999999999999999  |
+-------------+-------------------+------------------+--------------------+----------------------+


Boostrap metrics, sample 20 results:
+----------------+--------------------+--------------------+--------------------+
| accuracy_score |  precision_score   |    recall_score    |      f1_score      |
+----------------+--------------------+--------------------+--------------------+
|     0.915      | 0.937037037037037  | 0.9055555555555556 | 0.9102189925943575 |
|      0.91      | 0.9325842696629213 | 0.9016393442622951 | 0.9048076923076923 |
|     0.885      | 0.9290123456790124 | 0.8743169398907105 | 0.8828352610736031 |
|      0.85      | 0.9047619047619048 | 0.8648648648648649 |  0.85969868173258  |
|      0.9       | 0.933993399339934  | 0.8787878787878788 | 0.8892958892958892 |
|      0.86      | 0.899641577060932  | 0.8771929824561404 | 0.865659452837893  |
|     0.915      | 0.9470404984423676 | 0.8910256410256411 | 0.9061010949685903 |
|      0.86      | 0.9166666666666666 | 0.8444444444444444 | 0.8509316770186336 |
|      0.88      | 0.923076923076923  | 0.8787878787878788 | 0.8824476650563607 |
|     0.925      | 0.9500000000000001 |        0.9         | 0.9141494435612083 |
|      0.9       | 0.9305555555555555 | 0.8870056497175142 | 0.8932130991931656 |
|     0.925      | 0.951923076923077  | 0.912280701754386  | 0.9235882137436541 |
|     0.895      | 0.9306930693069306 |       0.875        | 0.8844028899277517 |
|      0.87      | 0.9182389937106917 | 0.8624338624338624 | 0.8667383512544804 |
|     0.885      | 0.9209621993127147 | 0.8743169398907105 | 0.8777246145667199 |
|     0.895      | 0.9363636363636364 | 0.8679245283018867 | 0.8824711794265445 |
|     0.875      | 0.9166666666666666 | 0.8538011695906432 | 0.8587479935794543 |
|      0.91      | 0.9454545454545454 |        0.9         | 0.9114735002912054 |
|      0.88      | 0.913978494623656  | 0.8873239436619719 | 0.8828206737811257 |
|      0.89      | 0.9294871794871794 | 0.8835978835978836 | 0.8900606561896884 |
+----------------+--------------------+--------------------+--------------------+

Trying to find weaknesses in the model by training models on the error:
Most important features by catboost:
+-------------+------------+
|   Feature   | Importance |
+-------------+------------+
| num_feat_20 |    18.0    |
| num_feat_15 |    16.0    |
|  num_feat_5 |    15.0    |
|  num_feat_3 |    14.0    |
| num_feat_16 |    14.0    |
| num_feat_19 |    8.0     |
|  num_feat_4 |    4.0     |
|  num_feat_9 |    4.0     |
|  num_feat_1 |    3.0     |
| num_feat_11 |    3.0     |
| num_feat_27 |    0.0     |
| num_feat_28 |    0.0     |
| num_feat_29 |    0.0     |
| num_feat_30 |    0.0     |
| num_feat_33 |    0.0     |
| num_feat_31 |    0.0     |
| num_feat_32 |    0.0     |
| num_feat_25 |    0.0     |
| num_feat_34 |    0.0     |
| num_feat_35 |    0.0     |
| num_feat_36 |    0.0     |
| num_feat_37 |    0.0     |
| num_feat_38 |    0.0     |
| num_feat_26 |    0.0     |
|  num_feat_0 |    0.0     |
| num_feat_24 |    0.0     |
| num_feat_23 |    0.0     |
| num_feat_22 |    0.0     |
| num_feat_21 |    0.0     |
| num_feat_18 |    0.0     |
| num_feat_17 |    0.0     |
| num_feat_14 |    0.0     |
| num_feat_13 |    0.0     |
| num_feat_12 |    0.0     |
| num_feat_10 |    0.0     |
|  num_feat_8 |    0.0     |
|  num_feat_7 |    0.0     |
|  num_feat_6 |    0.0     |
|  num_feat_2 |    0.0     |
| num_feat_39 |    0.0     |
+-------------+------------+

Print of tree to look for potential segments with high error (use with caution)
|--- num_feat_1 <= 58.16
|   |--- value: [0.50]
|--- num_feat_1 >  58.16
|   |--- value: [0.00]


